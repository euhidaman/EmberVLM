# EmberVLM Base Configuration
# Model architecture settings

model:
  name: "EmberVLM"
  total_params: 35_000_000  # ~35M total parameters

  # Vision Encoder: RepViT-XXS-M0.9 (frozen)
  vision_encoder:
    name: "RepViT-XXS-M0.9"
    type: "repvit_m0_9"
    input_size: 224
    output_dim: 384
    num_vision_tokens: 8  # Spatial pooling from 196 -> 8 tokens
    frozen: true
    pretrained: true

  # Language Model: TinyLLM-30M
  language_model:
    name: "TinyLLM-30M"
    hidden_size: 768
    num_layers: 6
    num_attention_heads: 12
    intermediate_size: 3072
    vocab_size: 50257  # GPT-2 tokenizer
    max_position_embeddings: 512
    dropout: 0.1
    # Partial fine-tuning settings
    freeze_layers: [0, 1, 2, 3]  # Freeze first 4 layers
    trainable_layers: [4, 5]     # Fine-tune last 2 layers

  # Multimodal Fusion Module (~500K params)
  fusion_module:
    input_dim: 384   # RepViT output
    output_dim: 768  # TinyLLM hidden size
    bottleneck_ratio: 16  # 1/16 for efficiency
    num_cross_attention_layers: 2
    dropout: 0.1

# Tokenizer settings
tokenizer:
  name: "gpt2"
  padding_side: "right"
  max_length: 512

# Quantization settings
quantization:
  enabled: true
  method: "gptq"  # Post-training quantization
  bits: 4
  group_size: 128
  calibration_samples: 512
  target_size_mb: 70

# Deployment settings
deployment:
  target_device: "raspberry_pi_zero"
  max_memory_mb: 400
  max_latency_ms: 500
  max_power_watts: 2
  gguf_enabled: true

