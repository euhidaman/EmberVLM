"""
EmberVLM Stage 3: Specialized Reasoning Finetuning
Robot selection and incident response with chain-of-thought reasoning.
"""

import os
import logging
from pathlib import Path
from typing import Dict, Optional, Any, Tuple, List
from dataclasses import dataclass
import random

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, ConcatDataset, WeightedRandomSampler
from torch.cuda.amp import autocast

from ..models import EmberVLM
from ..data import RobotSelectionDataset, IncidentDataset, GeneralVLMDataset
from .trainer import DistributedTrainer, TrainingConfig

logger = logging.getLogger(__name__)


@dataclass
class Stage3Config(TrainingConfig):
    """Configuration for Stage 3: Specialized Reasoning Finetuning."""

    # Stage-specific settings
    num_epochs: int = 3
    batch_size_per_gpu: int = 32
    gradient_accumulation_steps: int = 8
    learning_rate: float = 5e-5
    warmup_ratio: float = 0.05

    # Curriculum learning settings
    curriculum_enabled: bool = True
    epoch_1_difficulty: str = "single_robot"  # Easy
    epoch_2_difficulty: str = "multi_robot"   # Medium
    epoch_3_difficulty: str = "incident_response"  # Hard

    # Batch composition
    robot_selection_ratio: float = 0.5
    incident_response_ratio: float = 0.3
    general_vqa_ratio: float = 0.2

    # Loss weights
    cross_entropy_weight: float = 0.6
    reasoning_fidelity_weight: float = 0.25
    action_coherence_weight: float = 0.15

    # Chain-of-thought settings
    cot_enabled: bool = True
    max_reasoning_steps: int = 6

    # Data paths
    robot_selection_path: str = "Multi-Robot-Selection/multi_robot_selection_dataset.json"
    incident_data_dir: str = "incidents-dataset"

    def __post_init__(self):
        self.experiment_name = "stage3_reasoning"


class ReasoningFidelityLoss(nn.Module):
    """
    Loss for ensuring reasoning chains follow proper structure.
    Rewards proper use of <reasoning> and <answer> tags.
    """

    def __init__(self, tokenizer: Any = None):
        super().__init__()
        self.tokenizer = tokenizer

        # Special tokens for reasoning structure
        self.reasoning_start = "<reasoning>"
        self.reasoning_end = "</reasoning>"
        self.answer_start = "<answer>"
        self.answer_end = "</answer>"

    def forward(
        self,
        logits: torch.Tensor,
        labels: torch.Tensor,
        attention_mask: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute reasoning fidelity loss.

        Encourages model to:
        1. Start with <reasoning> tag
        2. Provide step-by-step analysis
        3. Conclude with <answer> tag
        """
        # Get predicted tokens
        predictions = logits.argmax(dim=-1)

        # This is a simplified version - in practice would decode and check structure
        # For now, use auxiliary cross-entropy on structure tokens

        # Standard CE loss but with emphasis on structure tokens
        loss = F.cross_entropy(
            logits.view(-1, logits.shape[-1]),
            labels.view(-1),
            ignore_index=-100,
            reduction='mean'
        )

        return loss


class ActionCoherenceLoss(nn.Module):
    """
    Loss for ensuring action plans are coherent and complete.
    """

    def __init__(self):
        super().__init__()

    def forward(
        self,
        hidden_states: torch.Tensor,
        action_labels: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Compute action coherence loss.

        Uses hidden state similarity to encourage consistent action planning.
        """
        # Compute self-similarity matrix
        hidden_norm = F.normalize(hidden_states, dim=-1)
        similarity = torch.matmul(hidden_norm, hidden_norm.transpose(-2, -1))

        # Coherence = smoothness of attention patterns
        # Adjacent tokens should have similar representations
        B, L, _ = hidden_states.shape

        if L > 1:
            diagonal_similarity = torch.diagonal(similarity, offset=1, dim1=-2, dim2=-1)
            coherence = diagonal_similarity.mean()
            loss = 1 - coherence  # Higher coherence = lower loss
        else:
            loss = torch.tensor(0.0, device=hidden_states.device)

        return loss


class CurriculumScheduler:
    """
    Curriculum learning scheduler for progressive difficulty.
    """

    def __init__(self, config: Stage3Config):
        self.config = config
        self.current_epoch = 0
        self.difficulties = {
            0: config.epoch_1_difficulty,
            1: config.epoch_2_difficulty,
            2: config.epoch_3_difficulty
        }

    def get_current_difficulty(self) -> str:
        """Get current difficulty level."""
        return self.difficulties.get(self.current_epoch, "incident_response")

    def get_batch_composition(self) -> Dict[str, float]:
        """Get batch composition based on current difficulty."""
        difficulty = self.get_current_difficulty()

        if difficulty == "single_robot":
            # Easy: Focus on simple single-robot tasks
            return {
                'robot_selection': 0.7,
                'incident': 0.1,
                'general': 0.2
            }
        elif difficulty == "multi_robot":
            # Medium: Multi-robot coordination
            return {
                'robot_selection': 0.5,
                'incident': 0.3,
                'general': 0.2
            }
        else:  # incident_response
            # Hard: Full incident response with dynamic replanning
            return {
                'robot_selection': 0.3,
                'incident': 0.5,
                'general': 0.2
            }

    def step_epoch(self):
        """Move to next epoch."""
        self.current_epoch += 1
        logger.info(f"Curriculum: Moving to epoch {self.current_epoch}, difficulty: {self.get_current_difficulty()}")


class Stage3ReasoningTrainer(DistributedTrainer):
    """
    Trainer for Stage 3: Specialized Reasoning Finetuning.

    Features:
    - Curriculum learning (easy â†’ hard)
    - Chain-of-thought reasoning
    - Robot selection optimization
    - Incident response planning
    """

    def __init__(
        self,
        model: EmberVLM,
        config: Stage3Config,
        train_datasets: Dict[str, Any],
        eval_datasets: Optional[Dict[str, Any]] = None,
        tokenizer: Any = None,
        **kwargs
    ):
        # Store datasets before creating dataloaders
        self.train_datasets = train_datasets
        self.eval_datasets = eval_datasets or {}
        self.tokenizer = tokenizer

        # Initialize curriculum scheduler
        self.curriculum = CurriculumScheduler(config) if config.curriculum_enabled else None

        # Create initial dataloader with current batch composition
        train_dataloader = self._create_mixed_dataloader(
            train_datasets,
            config.batch_size_per_gpu,
            self.curriculum.get_batch_composition() if self.curriculum else {
                'robot_selection': config.robot_selection_ratio,
                'incident': config.incident_response_ratio,
                'general': config.general_vqa_ratio
            }
        )

        eval_dataloader = self._create_mixed_dataloader(
            eval_datasets,
            config.batch_size_per_gpu,
            {'robot_selection': 0.5, 'incident': 0.3, 'general': 0.2}
        ) if eval_datasets else None

        super().__init__(
            model=model,
            config=config,
            train_dataloader=train_dataloader,
            eval_dataloader=eval_dataloader,
            **kwargs
        )

        self.stage_config = config

        # Reasoning-specific losses
        self.reasoning_loss = ReasoningFidelityLoss(tokenizer)
        self.coherence_loss = ActionCoherenceLoss()

        # Metrics tracking
        self.robot_selection_acc = []
        self.incident_response_quality = []

    def _create_mixed_dataloader(
        self,
        datasets: Dict[str, Any],
        batch_size: int,
        composition: Dict[str, float]
    ) -> Optional[DataLoader]:
        """Create dataloader with weighted sampling from multiple datasets."""
        if not datasets:
            return None

        all_samples = []
        weights = []

        total_weight = sum(composition.values())

        for name, dataset in datasets.items():
            if dataset is None:
                continue

            ratio = composition.get(name, 0.1) / total_weight
            weight_per_sample = ratio / len(dataset) if len(dataset) > 0 else 0

            for i in range(len(dataset)):
                all_samples.append((name, i))
                weights.append(weight_per_sample)

        if not all_samples:
            return None

        # Create wrapper dataset
        class MixedDataset(torch.utils.data.Dataset):
            def __init__(self, samples, datasets):
                self.samples = samples
                self.datasets = datasets

            def __len__(self):
                return len(self.samples)

            def __getitem__(self, idx):
                name, sample_idx = self.samples[idx]
                item = self.datasets[name][sample_idx]
                item['task_type'] = name
                return item

        mixed_dataset = MixedDataset(all_samples, datasets)

        sampler = WeightedRandomSampler(
            weights=weights,
            num_samples=len(all_samples),
            replacement=True
        )

        return DataLoader(
            mixed_dataset,
            batch_size=batch_size,
            sampler=sampler,
            num_workers=4,
            pin_memory=True
        )

    def compute_loss(
        self,
        batch: Dict[str, torch.Tensor]
    ) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        Compute combined loss for reasoning tasks.
        """
        metrics = {}
        total_loss = torch.tensor(0.0, device=self.device)

        # Forward pass
        outputs = self.model(
            pixel_values=batch.get('pixel_values'),
            input_ids=batch.get('input_ids'),
            attention_mask=batch.get('attention_mask'),
            labels=batch.get('labels'),
            output_hidden_states=True
        )

        # 1. Cross-entropy loss
        if 'loss' in outputs:
            ce_loss = outputs['loss']
            total_loss = total_loss + self.stage_config.cross_entropy_weight * ce_loss
            metrics['ce_loss'] = ce_loss.item()

        # 2. Reasoning fidelity loss
        if self.stage_config.cot_enabled:
            reasoning_loss = self.reasoning_loss(
                outputs['logits'],
                batch.get('labels'),
                batch.get('attention_mask')
            )
            total_loss = total_loss + self.stage_config.reasoning_fidelity_weight * reasoning_loss
            metrics['reasoning_loss'] = reasoning_loss.item()

        # 3. Action coherence loss
        hidden_states = outputs.get('hidden_states', [])
        if hidden_states:
            coherence_loss = self.coherence_loss(hidden_states[-1])
            total_loss = total_loss + self.stage_config.action_coherence_weight * coherence_loss
            metrics['coherence_loss'] = coherence_loss.item()

        metrics['total_loss'] = total_loss.item()

        return total_loss, metrics

    def on_epoch_end(self):
        """Callback at end of each epoch for curriculum update."""
        if self.curriculum:
            self.curriculum.step_epoch()

            # Recreate dataloader with new composition
            new_composition = self.curriculum.get_batch_composition()
            self.train_dataloader = self._create_mixed_dataloader(
                self.train_datasets,
                self.config.batch_size_per_gpu,
                new_composition
            )

            logger.info(f"Updated batch composition: {new_composition}")

    def evaluate(self) -> Dict[str, float]:
        """
        Comprehensive evaluation for reasoning tasks.
        """
        if not self.eval_dataloader:
            return {}

        self.model.eval()

        metrics = {
            'robot_selection_correct': 0,
            'robot_selection_total': 0,
            'incident_response_correct': 0,
            'incident_response_total': 0,
            'general_loss': 0.0,
            'general_count': 0
        }

        with torch.no_grad():
            for batch in self.eval_dataloader:
                batch = self._prepare_batch(batch)
                task_types = batch.get('task_type', ['general'] * batch['input_ids'].shape[0])

                with autocast(device_type='cuda', dtype=self.autocast_dtype):
                    outputs = self.model(
                        pixel_values=batch.get('pixel_values'),
                        input_ids=batch.get('input_ids'),
                        attention_mask=batch.get('attention_mask'),
                        labels=batch.get('labels')
                    )

                # Task-specific metrics
                for i, task_type in enumerate(task_types):
                    if task_type == 'robot_selection':
                        # Check robot selection accuracy
                        metrics['robot_selection_total'] += 1
                        # Simplified accuracy check
                        if outputs.get('loss') is not None and outputs['loss'].item() < 2.0:
                            metrics['robot_selection_correct'] += 1

                    elif task_type == 'incident':
                        metrics['incident_response_total'] += 1
                        if outputs.get('loss') is not None and outputs['loss'].item() < 2.0:
                            metrics['incident_response_correct'] += 1

                    else:
                        if outputs.get('loss') is not None:
                            metrics['general_loss'] += outputs['loss'].item()
                            metrics['general_count'] += 1

        # Compute final metrics
        eval_metrics = {}

        if metrics['robot_selection_total'] > 0:
            eval_metrics['robot_selection_accuracy'] = \
                metrics['robot_selection_correct'] / metrics['robot_selection_total']

        if metrics['incident_response_total'] > 0:
            eval_metrics['incident_response_accuracy'] = \
                metrics['incident_response_correct'] / metrics['incident_response_total']

        if metrics['general_count'] > 0:
            eval_metrics['general_loss'] = metrics['general_loss'] / metrics['general_count']

        return eval_metrics

    def train(self) -> Dict[str, float]:
        """Training loop with curriculum callbacks."""
        results = {}

        for epoch in range(self.config.num_epochs):
            logger.info(f"Starting epoch {epoch + 1}/{self.config.num_epochs}")

            if self.curriculum:
                difficulty = self.curriculum.get_current_difficulty()
                logger.info(f"Current difficulty: {difficulty}")

            # Train for one epoch
            epoch_results = super().train()
            results.update(epoch_results)

            # End of epoch callback
            self.on_epoch_end()

        return results


def run_stage3_reasoning(
    model: EmberVLM,
    robot_selection_path: str,
    incident_data_dir: str,
    eval_robot_selection_path: Optional[str] = None,
    eval_incident_data_dir: Optional[str] = None,
    config: Optional[Stage3Config] = None,
    tokenizer: Any = None,
    **kwargs
) -> Dict[str, float]:
    """
    Run Stage 3: Specialized Reasoning Finetuning.

    Args:
        model: EmberVLM model
        robot_selection_path: Path to robot selection dataset
        incident_data_dir: Directory with incident data
        eval_robot_selection_path: Optional evaluation robot selection path
        eval_incident_data_dir: Optional evaluation incident directory
        config: Stage 3 configuration
        tokenizer: Tokenizer for text processing

    Returns:
        Training metrics
    """
    if config is None:
        config = Stage3Config(**kwargs)

    logger.info("=" * 50)
    logger.info("Stage 3: Specialized Reasoning Finetuning")
    logger.info("=" * 50)
    logger.info(f"Curriculum learning: {config.curriculum_enabled}")
    logger.info(f"Chain-of-thought: {config.cot_enabled}")
    logger.info(f"Batch composition:")
    logger.info(f"  Robot selection: {config.robot_selection_ratio}")
    logger.info(f"  Incident response: {config.incident_response_ratio}")
    logger.info(f"  General VQA: {config.general_vqa_ratio}")

    # Load datasets
    train_datasets = {}
    eval_datasets = {}

    # Robot selection dataset
    if Path(robot_selection_path).exists():
        train_datasets['robot_selection'] = RobotSelectionDataset(
            data_path=robot_selection_path,
            split='train',
            augment=True,
            tokenizer=tokenizer
        )
        logger.info(f"Loaded robot selection dataset: {len(train_datasets['robot_selection'])} samples")

        if eval_robot_selection_path and Path(eval_robot_selection_path).exists():
            eval_datasets['robot_selection'] = RobotSelectionDataset(
                data_path=eval_robot_selection_path,
                split='val',
                augment=False,
                tokenizer=tokenizer
            )
    else:
        logger.warning(f"Robot selection data not found: {robot_selection_path}")

    # Incident dataset
    if Path(incident_data_dir).exists():
        train_datasets['incident'] = IncidentDataset(
            data_dir=incident_data_dir,
            split='train',
            tokenizer=tokenizer
        )
        logger.info(f"Loaded incident dataset: {len(train_datasets['incident'])} samples")

        if eval_incident_data_dir and Path(eval_incident_data_dir).exists():
            eval_datasets['incident'] = IncidentDataset(
                data_dir=eval_incident_data_dir,
                split='val',
                tokenizer=tokenizer
            )
    else:
        logger.warning(f"Incident data not found: {incident_data_dir}")

    # General VQA dataset (synthetic for now)
    train_datasets['general'] = GeneralVLMDataset(
        dataset_name='vqa',
        split='train',
        max_samples=10000
    )

    if not train_datasets:
        raise ValueError("No training data available!")

    # Create trainer
    trainer = Stage3ReasoningTrainer(
        model=model,
        config=config,
        train_datasets=train_datasets,
        eval_datasets=eval_datasets if eval_datasets else None,
        tokenizer=tokenizer
    )

    # Train
    results = trainer.train()

    logger.info("Stage 3 complete!")
    logger.info(f"Final results: {results}")

    return results


if __name__ == "__main__":
    # Test Stage 3 components
    print("Testing Stage 3: Specialized Reasoning Finetuning...")

    config = Stage3Config(
        output_dir="test_outputs",
        num_epochs=3,
        batch_size_per_gpu=4,
        curriculum_enabled=True
    )

    print(f"Stage 3 Config: {config}")

    # Test curriculum scheduler
    curriculum = CurriculumScheduler(config)

    for epoch in range(3):
        difficulty = curriculum.get_current_difficulty()
        composition = curriculum.get_batch_composition()
        print(f"Epoch {epoch + 1}: difficulty={difficulty}, composition={composition}")
        curriculum.step_epoch()

    # Test losses
    reasoning_loss = ReasoningFidelityLoss()
    coherence_loss = ActionCoherenceLoss()

    logits = torch.randn(4, 32, 1000)
    labels = torch.randint(0, 1000, (4, 32))
    attention_mask = torch.ones(4, 32)

    r_loss = reasoning_loss(logits, labels, attention_mask)
    print(f"Reasoning loss: {r_loss.item():.4f}")

    hidden_states = torch.randn(4, 32, 768)
    c_loss = coherence_loss(hidden_states)
    print(f"Coherence loss: {c_loss.item():.4f}")

    print("Stage 3 tests complete!")

